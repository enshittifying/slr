# Quality Assurance & Testing Analysis Report
## SLR Citation Processor - Comprehensive Testing Strategy

**Generated:** 2025-11-16
**Analyst:** QA & Testing Expert
**Project:** Stanford Law Review Citation Processor
**Current Test Coverage:** ~40% (Estimated)
**Target Coverage:** 80%+

---

## Executive Summary

### Current State
- **Total Production Code:** ~20,000 lines across 180 Python files
- **Existing Tests:** 41 test functions in 2 test files
- **Test Coverage:** ~40% estimated (actual measurement needed)
- **Test Infrastructure:** pytest + pytest-cov + pytest-mock + pytest-qt installed
- **Quality Scripts:** 2 verification scripts (verify_modules.py, verify_integration.py)

### Critical Gaps Identified
1. âŒ **No unit tests** for utility modules (8 modules untested)
2. âŒ **No GUI tests** despite PyQt6 application
3. âŒ **No data layer tests** for API clients (Sheets, Drive, LLM)
4. âŒ **No performance/load tests**
5. âŒ **No E2E tests** for full workflows
6. âŒ **No CI/CD pipeline** for automated testing
7. âŒ **No test configuration** (pytest.ini, conftest.py)
8. âŒ **No coverage measurement** in place

---

## 1. Test Coverage Analysis

### 1.1 Current Test Coverage by Component

| Component | Files | Lines | Tests | Coverage | Status |
|-----------|-------|-------|-------|----------|--------|
| **Core Pipeline** | 4 | 1,351 | 15 | ~60% | ğŸŸ¡ Partial |
| **Data Layer** | 5 | 1,700 | 0 | 0% | ğŸ”´ Missing |
| **GUI Layer** | 7 | 1,765 | 0 | 0% | ğŸ”´ Missing |
| **Utils** | 8 | 2,050 | 0 | 0% | ğŸ”´ Missing |
| **SLRinator/src/core** | 7 | 4,200 | 0 | 0% | ğŸ”´ Missing |
| **SLRinator/src/utils** | 8 | 3,100 | 6 | ~15% | ğŸ”´ Critical |
| **SLRinator/src/stage1** | 6 | 2,500 | 0 | 0% | ğŸ”´ Missing |
| **SLRinator/src/processors** | 2 | 1,200 | 0 | 0% | ğŸ”´ Missing |
| **R2 Pipeline** | 14 | 2,834 | 20 | ~50% | ğŸŸ¡ Partial |
| **Integration Tests** | - | - | 6 | - | ğŸŸ¢ Good |
| **TOTAL** | 180+ | 20,000+ | 41 | ~40% | ğŸ”´ Insufficient |

### 1.2 Critical Untested Paths

#### **High-Risk Untested Components:**

1. **Error Handling & Recovery** (`SLRinator/src/utils/error_handler.py`)
   - âŒ No tests for ErrorContext creation
   - âŒ No tests for recovery strategies
   - âŒ No tests for error history tracking
   - âŒ No tests for retry mechanisms
   - **Risk:** Silent failures, unhandled errors in production

2. **Cache Management** (`SLRinator/src/utils/cache_manager.py`)
   - âŒ No tests for cache operations (get/set/delete)
   - âŒ No tests for expiration logic
   - âŒ No tests for corruption detection
   - âŒ No tests for SQLite database operations
   - **Risk:** Data corruption, memory leaks, stale cache issues

3. **API Clients** (`app/data/`)
   - âŒ No tests for Google Sheets client (270 lines)
   - âŒ No tests for Google Drive client (320 lines)
   - âŒ No tests for LLM client (250 lines)
   - âŒ No tests for retry logic (11 critical API methods)
   - **Risk:** API failures, rate limiting issues, credential problems

4. **PDF Processing** (`SLRinator/src/core/pdf_retriever.py`)
   - âŒ No tests for PDF download/retrieval (759 lines)
   - âŒ No tests for PDF validation
   - âŒ No tests for corruption handling
   - **Risk:** Failed retrievals, corrupted PDFs, format issues

5. **Citation Parsing** (`SLRinator/src/core/gpt_citation_parser.py`)
   - âŒ No tests for GPT-based parsing
   - âŒ No tests for fallback parsing
   - âŒ No tests for edge cases (malformed citations)
   - **Risk:** Incorrect citation extraction, API failures

6. **GUI Components** (`app/gui/`)
   - âŒ No tests for any PyQt6 widgets (7 modules, 1,765 lines)
   - âŒ No tests for user interactions
   - âŒ No tests for progress updates
   - **Risk:** UI crashes, broken workflows, UX issues

---

## 2. Testing Strategy

### 2.1 Test Pyramid Structure

```
              â•±â”€â”€â”€â”€â”€â”€â”€â”€â•²
             â•±   E2E    â•²     10% - Full workflow tests
            â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
           â•± Integration  â•²   20% - Component integration
          â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
         â•±   Unit Tests     â•²  70% - Individual functions/classes
        â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
```

### 2.2 Recommended Test Distribution

| Test Type | Current | Target | Priority |
|-----------|---------|--------|----------|
| Unit Tests | 15 | 200+ | ğŸ”´ Critical |
| Integration Tests | 6 | 30+ | ğŸŸ¡ High |
| GUI Tests | 0 | 20+ | ğŸŸ¡ High |
| E2E Tests | 0 | 10+ | ğŸŸ¢ Medium |
| Performance Tests | 0 | 5+ | ğŸŸ¢ Medium |
| Regression Tests | 0 | Suite | ğŸŸ¢ Medium |

### 2.3 Test Categories Needed

#### **A. Unit Tests (70% of test suite)**

1. **Utility Functions**
   - Error handler (retry, fallback, recovery)
   - Cache manager (CRUD, expiration, corruption)
   - Retry handler (exponential backoff, circuit breaker)
   - Connection pool (rate limiting, pooling)
   - Performance monitor (metrics, statistics)
   - API logger (logging, rotation)

2. **Data Models**
   - Citation models (serialization, validation)
   - Article/Source models (state transitions)
   - Pipeline stage enums
   - Validation result models

3. **Business Logic**
   - Citation parser (GPT + fallback)
   - PDF retriever (download, validation)
   - Source identifier (classification)
   - Footnote processor
   - Quote checker

#### **B. Integration Tests (20% of test suite)**

1. **Data Layer Integration**
   - Google Sheets â†” Application
   - Google Drive â†” Application
   - LLM API â†” Application
   - Cache â†” Application

2. **Pipeline Integration**
   - SP â†’ R1 handoff
   - R1 â†’ R2 handoff
   - Full SP â†’ R1 â†’ R2 flow
   - Error recovery across stages

3. **External API Integration**
   - CourtListener API
   - GovInfo API
   - OpenAI API
   - Anthropic API

#### **C. GUI Tests (10% of test suite)**

1. **Widget Tests**
   - Main window initialization
   - Tab switching
   - Settings dialog
   - Progress widgets
   - Error dialogs

2. **User Interaction Tests**
   - Button clicks
   - File uploads
   - Article selection
   - Configuration changes

3. **Worker Thread Tests**
   - Background processing
   - Progress callbacks
   - Error handling in threads

#### **D. E2E Tests (5% of test suite)**

1. **Full Workflow Tests**
   - New article processing (SP â†’ R1 â†’ R2)
   - Resume from cache
   - Error recovery
   - Multi-article batch processing

2. **Edge Case Scenarios**
   - Network failures during processing
   - Corrupted PDF handling
   - Missing credentials
   - API rate limiting

---

## 3. Quality Metrics & Targets

### 3.1 Coverage Targets

| Metric | Current | 3-Month | 6-Month | Target |
|--------|---------|---------|---------|--------|
| **Line Coverage** | ~40% | 65% | 75% | 80%+ |
| **Branch Coverage** | Unknown | 60% | 70% | 75%+ |
| **Function Coverage** | ~35% | 70% | 80% | 85%+ |
| **Critical Path Coverage** | ~60% | 90% | 95% | 100% |

### 3.2 Code Quality Gates

#### **Pre-Commit Checks:**
- âœ… All tests pass
- âœ… Coverage â‰¥ 80% (target)
- âœ… No linting errors (flake8, pylint)
- âœ… Type hints validated (mypy)
- âœ… Code formatted (black)

#### **Pre-Merge Checks:**
- âœ… Integration tests pass
- âœ… No regression failures
- âœ… Performance benchmarks met
- âœ… Security scan clean

#### **Pre-Release Checks:**
- âœ… E2E tests pass
- âœ… Manual QA sign-off
- âœ… Load tests pass
- âœ… Documentation updated

### 3.3 Quality Metrics to Track

```python
# Automated Quality Dashboard
{
    "test_execution": {
        "total_tests": 250,      # Target
        "passed": 248,
        "failed": 2,
        "skipped": 0,
        "duration_seconds": 45
    },
    "coverage": {
        "line_coverage": 82.5,
        "branch_coverage": 76.3,
        "function_coverage": 87.1,
        "critical_path_coverage": 98.2
    },
    "code_quality": {
        "complexity_score": 7.2,  # Cyclomatic complexity
        "maintainability_index": 85,
        "technical_debt_hours": 12,
        "code_smells": 5
    },
    "performance": {
        "avg_test_duration_ms": 180,
        "slowest_test_ms": 2500,
        "memory_usage_mb": 256,
        "cpu_usage_percent": 45
    }
}
```

---

## 4. Testing Gaps Analysis

### 4.1 Critical Missing Test Categories

#### **1. Error Scenario Testing** âŒ

**Missing Tests:**
- Network failures (connection timeout, DNS errors, SSL errors)
- API failures (rate limiting, quota exceeded, invalid credentials)
- File system errors (disk full, permission denied, corrupted files)
- Data validation errors (malformed citations, invalid PDF, bad JSON)
- Concurrent access errors (race conditions, deadlocks)

**Impact:** High - Production failures won't be caught

#### **2. Edge Case Testing** âŒ

**Missing Tests:**
- Empty inputs (empty citation, empty PDF, no sources)
- Boundary values (max file size, max citations, API limits)
- Unicode/encoding issues (non-ASCII characters, emoji, special chars)
- SQL injection attempts (in citation text)
- XSS attempts (in user inputs)
- Path traversal attempts (in filenames)

**Impact:** High - Security vulnerabilities, data corruption

#### **3. Performance Testing** âŒ

**Missing Tests:**
- Load testing (100+ sources per article)
- Stress testing (10 concurrent articles)
- Memory leak testing (long-running processes)
- API rate limiting (burst requests)
- Database query optimization
- Cache hit/miss ratios

**Impact:** Medium - Performance degradation in production

#### **4. GUI Testing** âŒ

**Missing Tests:**
- Accessibility testing (keyboard navigation, screen readers)
- Cross-platform testing (Mac vs Windows)
- Responsive layout testing (different screen sizes)
- Long-running operation UX (progress bars, cancellation)
- Error dialog presentation
- Settings persistence

**Impact:** Medium - Poor user experience

#### **5. Regression Testing** âŒ

**Missing Tests:**
- No regression test suite
- No test data versioning
- No baseline performance metrics
- No compatibility testing (Python versions, dependencies)

**Impact:** High - Breaking changes go undetected

### 4.2 API Integration Testing Gaps

| API | Endpoints Used | Tests Needed | Current Coverage |
|-----|---------------|--------------|------------------|
| **Google Sheets** | 5 methods | 15 tests | 0% âŒ |
| **Google Drive** | 3 methods | 10 tests | 0% âŒ |
| **OpenAI** | 2 methods | 8 tests | 0% âŒ |
| **Anthropic** | 2 methods | 8 tests | 0% âŒ |
| **CourtListener** | 3 endpoints | 10 tests | 0% âŒ |
| **GovInfo** | 2 endpoints | 6 tests | 0% âŒ |

**Total API Tests Needed:** 57 tests
**Current Coverage:** 0 tests

---

## 5. Test Fixture Recommendations

### 5.1 Core Test Fixtures

```python
# /home/user/slr/tests/conftest.py

import pytest
import tempfile
import shutil
from pathlib import Path
from unittest.mock import Mock, MagicMock
from typing import Dict, Any

@pytest.fixture
def temp_cache_dir():
    """Temporary cache directory for tests"""
    temp_dir = tempfile.mkdtemp()
    yield Path(temp_dir)
    shutil.rmtree(temp_dir, ignore_errors=True)

@pytest.fixture
def mock_sheets_client():
    """Mock Google Sheets client"""
    client = Mock()
    client.get_all_articles.return_value = [
        {
            'article_id': 'test-001',
            'volume_issue': '78.6',
            'author': 'Test Author',
            'title': 'Test Article',
            'stage': 'not_started',
            'sources_total': 10,
            'sources_completed': 0
        }
    ]
    client.get_sources_for_article.return_value = [
        {
            'source_id': f'SP-{i:03d}',
            'article_id': 'test-001',
            'footnote_num': str(i),
            'citation': f'Test Citation {i}',
            'type': 'case',
            'status': 'pending'
        }
        for i in range(1, 11)
    ]
    return client

@pytest.fixture
def mock_drive_client():
    """Mock Google Drive client"""
    client = Mock()
    client.upload_file.return_value = 'file-123'
    client.get_file_link.return_value = 'https://drive.google.com/file/d/file-123'
    client.download_file.return_value = '/tmp/test.pdf'
    return client

@pytest.fixture
def mock_llm_client():
    """Mock LLM client"""
    client = Mock()
    client.check_format.return_value = {
        'issues': [],
        'suggestion': '',
        'confidence': 95
    }
    client.check_support.return_value = {
        'supported': True,
        'confidence': 90,
        'explanation': 'Source supports the proposition'
    }
    return client

@pytest.fixture
def sample_citation_data():
    """Sample citation data for testing"""
    return {
        'valid_case': 'Alice Corp. v. CLS Bank, 573 U.S. 208 (2014)',
        'valid_statute': '35 U.S.C. Â§ 101 (2018)',
        'valid_article': 'John Doe, Patent Law, 100 Harv. L. Rev. 123 (2020)',
        'malformed': 'Bad Citation;;;',
        'empty': '',
        'unicode': 'MÃ¼ller v. SociÃ©tÃ©, 123 F.3d 456 (2020)',
        'sql_injection': "'; DROP TABLE citations; --",
        'xss': '<script>alert("xss")</script>',
    }

@pytest.fixture
def sample_pdf(temp_cache_dir):
    """Create a sample PDF for testing"""
    import fitz
    doc = fitz.open()
    page = doc.new_page()
    page.insert_text((72, 72), "Test PDF Content")
    pdf_path = temp_cache_dir / "test.pdf"
    doc.save(pdf_path)
    doc.close()
    return pdf_path

@pytest.fixture
def sample_article_data():
    """Sample article with footnotes"""
    return {
        'article_id': 'test-001',
        'title': 'Test Article on Patent Law',
        'author': 'John Doe',
        'volume_issue': '78.6',
        'footnotes': [
            {
                'number': 1,
                'text': 'See Alice Corp. v. CLS Bank, 573 U.S. 208, 215 (2014).',
                'citation': 'Alice Corp. v. CLS Bank, 573 U.S. 208 (2014)',
                'page': '215'
            },
            {
                'number': 2,
                'text': '35 U.S.C. Â§ 101 (2018).',
                'citation': '35 U.S.C. Â§ 101 (2018)',
                'page': None
            }
        ]
    }

@pytest.fixture(autouse=True)
def reset_singletons():
    """Reset singleton instances between tests"""
    # Reset any singleton caches
    yield
    # Cleanup code here

@pytest.fixture
def mock_config():
    """Mock configuration"""
    return {
        'google': {
            'spreadsheet_id': 'test-sheet-123',
            'drive_folder_id': 'test-folder-456',
            'credentials_path': '/tmp/test-creds.json'
        },
        'llm': {
            'provider': 'openai',
            'model': 'gpt-4o-mini',
            'api_key': 'test-key-789'
        },
        'paths': {
            'cache_dir': '/tmp/test-cache',
            'logs_dir': '/tmp/test-logs'
        }
    }
```

### 5.2 Test Data Management

**Directory Structure:**
```
tests/
â”œâ”€â”€ conftest.py                 # Global fixtures
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ sample_citations.json   # Citation test data
â”‚   â”œâ”€â”€ sample_pdfs/           # PDF test files
â”‚   â”œâ”€â”€ sample_articles/       # Word docs for testing
â”‚   â””â”€â”€ api_responses/         # Mocked API responses
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_utils/
â”‚   â”œâ”€â”€ test_core/
â”‚   â”œâ”€â”€ test_data/
â”‚   â””â”€â”€ test_processors/
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_pipeline/
â”‚   â”œâ”€â”€ test_api_clients/
â”‚   â””â”€â”€ test_cache/
â”œâ”€â”€ gui/
â”‚   â”œâ”€â”€ test_widgets/
â”‚   â””â”€â”€ test_workflows/
â””â”€â”€ e2e/
    â””â”€â”€ test_full_pipeline/
```

---

## 6. CI/CD Pipeline Test Strategy

### 6.1 Automated Test Pipeline

```yaml
# .github/workflows/test.yml

name: Test Suite

on:
  push:
    branches: [main, develop, claude/*]
  pull_request:
    branches: [main, develop]

jobs:
  unit-tests:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: [3.11, 3.12]

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-mock pytest-qt

    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=app --cov=SLRinator \
          --cov-report=xml --cov-report=html

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage.xml

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: pip install -r requirements.txt

    - name: Run integration tests
      run: pytest tests/integration/ -v --timeout=300

    - name: Archive test artifacts
      if: failure()
      uses: actions/upload-artifact@v3
      with:
        name: test-failures
        path: |
          logs/
          cache/

  gui-tests:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y xvfb libxkbcommon-x11-0

    - name: Install dependencies
      run: pip install -r requirements.txt

    - name: Run GUI tests (Linux)
      if: matrix.os == 'ubuntu-latest'
      run: xvfb-run pytest tests/gui/ -v

    - name: Run GUI tests (Mac)
      if: matrix.os == 'macos-latest'
      run: pytest tests/gui/ -v

  e2e-tests:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: pip install -r requirements.txt

    - name: Run E2E tests
      run: pytest tests/e2e/ -v --timeout=600
      env:
        GOOGLE_SHEETS_ID: ${{ secrets.TEST_SHEETS_ID }}
        GOOGLE_DRIVE_FOLDER: ${{ secrets.TEST_DRIVE_FOLDER }}
        OPENAI_API_KEY: ${{ secrets.TEST_OPENAI_KEY }}

  code-quality:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        pip install black flake8 mypy pylint

    - name: Run black
      run: black --check app/ SLRinator/ r2_pipeline/

    - name: Run flake8
      run: flake8 app/ SLRinator/ r2_pipeline/ --max-line-length=100

    - name: Run mypy
      run: mypy app/ --ignore-missing-imports

    - name: Run pylint
      run: pylint app/ --fail-under=8.0

  verification-scripts:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Run module verification
      run: python verify_modules.py

    - name: Run integration verification
      run: python verify_integration.py
```

### 6.2 Pre-Commit Hooks

```bash
# .pre-commit-config.yaml

repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-json
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black
        language_version: python3.11

  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        args: [--max-line-length=100]

  - repo: local
    hooks:
      - id: pytest-quick
        name: Run quick tests
        entry: pytest tests/unit/ -v --timeout=10 -m "not slow"
        language: system
        pass_filenames: false
        always_run: true
```

---

## 7. Specific Test Cases to Add

### 7.1 Priority 1: Critical Utility Tests

**File:** `/home/user/slr/tests/unit/test_utils/test_error_handler.py`

```python
"""Tests for error handling and recovery"""
import pytest
from SLRinator.src.utils.error_handler import (
    ErrorHandler, ErrorSeverity, RecoveryStrategy, ErrorContext
)

class TestErrorHandler:
    """Test error handler functionality"""

    def test_handle_connection_error(self):
        """Test handling of network errors"""
        handler = ErrorHandler(max_retries=3)
        error = ConnectionError("Connection refused")

        context = handler.handle_error(error, "test_operation")

        assert context.error_type == "ConnectionError"
        assert context.severity == ErrorSeverity.HIGH
        assert context.recovery_strategy == RecoveryStrategy.RETRY

    def test_retry_with_exponential_backoff(self):
        """Test retry mechanism with backoff"""
        handler = ErrorHandler(max_retries=3)
        attempt_count = 0

        def flaky_operation():
            nonlocal attempt_count
            attempt_count += 1
            if attempt_count < 3:
                raise ConnectionError("Temporary failure")
            return "Success"

        context = handler.handle_error(ConnectionError("Test"), "retry_test")
        result = handler.recover(context, retry_func=flaky_operation)

        assert result == "Success"
        assert attempt_count == 3

    def test_fallback_strategy(self):
        """Test fallback recovery strategy"""
        handler = ErrorHandler()

        def primary_operation():
            raise FileNotFoundError("File missing")

        def fallback_operation():
            return "Fallback result"

        context = handler.handle_error(
            FileNotFoundError("Test"), "fallback_test"
        )
        result = handler.recover(
            context,
            retry_func=primary_operation,
            fallback_func=fallback_operation
        )

        assert result == "Fallback result"

    def test_error_history_tracking(self):
        """Test error history is maintained"""
        handler = ErrorHandler()

        for i in range(5):
            handler.handle_error(ValueError(f"Error {i}"), f"op_{i}")

        assert len(handler.error_history) == 5
        summary = handler.get_error_summary()
        assert summary['total_errors'] == 5

    def test_save_error_report(self, tmp_path):
        """Test error report generation"""
        handler = ErrorHandler(log_file=str(tmp_path / "errors.log"))
        handler.handle_error(RuntimeError("Test error"), "test_op")

        report_path = tmp_path / "error_report.json"
        handler.save_error_report(str(report_path))

        assert report_path.exists()
        # Verify JSON structure
```

**File:** `/home/user/slr/tests/unit/test_utils/test_cache_manager.py`

```python
"""Tests for cache management"""
import pytest
import time
from datetime import timedelta
from SLRinator.src.utils.cache_manager import CacheManager, CacheEntry

class TestCacheManager:
    """Test cache manager functionality"""

    def test_cache_set_and_get(self, tmp_path):
        """Test basic cache operations"""
        cache = CacheManager(cache_dir=str(tmp_path), max_size_mb=10)

        cache.set('key1', {'data': 'test'})
        result = cache.get('key1')

        assert result == {'data': 'test'}
        assert cache.stats['hits'] == 1

    def test_cache_expiration(self, tmp_path):
        """Test cache entry expiration"""
        cache = CacheManager(cache_dir=str(tmp_path))

        # Set with 1 second TTL
        cache.set('key1', 'data', ttl=timedelta(seconds=1))

        # Should exist immediately
        assert cache.get('key1') == 'data'

        # Should expire after TTL
        time.sleep(1.5)
        assert cache.get('key1') is None

    def test_cache_corruption_detection(self, tmp_path):
        """Test detection of corrupted cache entries"""
        cache = CacheManager(cache_dir=str(tmp_path))

        cache.set('key1', {'data': 'test'})

        # Manually corrupt the cache file
        cache_file = list(tmp_path.glob('*.cache'))[0]
        with open(cache_file, 'wb') as f:
            f.write(b'corrupted data')

        # Should detect corruption and return None
        result = cache.get('key1')
        assert result is None
        assert cache.stats['corrupted'] == 1

    def test_cache_size_limit(self, tmp_path):
        """Test cache size management"""
        cache = CacheManager(cache_dir=str(tmp_path), max_size_mb=1)

        # Add entries until size limit
        large_data = 'x' * (500 * 1024)  # 500KB
        for i in range(5):
            cache.set(f'key{i}', large_data)

        stats = cache.get_statistics()
        # Should have evicted some entries
        assert stats['total_entries'] < 5
        assert cache.stats['evictions'] > 0

    def test_cache_statistics(self, tmp_path):
        """Test cache statistics tracking"""
        cache = CacheManager(cache_dir=str(tmp_path))

        cache.set('key1', 'data1')
        cache.set('key2', 'data2')
        cache.get('key1')  # Hit
        cache.get('key3')  # Miss

        stats = cache.get_statistics()
        assert stats['total_entries'] == 2
        assert stats['hit_rate'] == 0.5  # 1 hit, 1 miss
```

### 7.2 Priority 2: Data Layer Tests

**File:** `/home/user/slr/tests/unit/test_data/test_sheets_client.py`

```python
"""Tests for Google Sheets client"""
import pytest
from unittest.mock import Mock, patch, MagicMock
from app.data.sheets_client import SheetsClient

class TestSheetsClient:
    """Test Google Sheets client"""

    @pytest.fixture
    def mock_credentials(self, tmp_path):
        """Mock service account credentials"""
        creds_file = tmp_path / "credentials.json"
        creds_file.write_text('{"type": "service_account"}')
        return str(creds_file)

    @pytest.fixture
    def sheets_client(self, mock_credentials):
        """Create sheets client with mocked service"""
        with patch('app.data.sheets_client.service_account'):
            client = SheetsClient(
                credentials_path=mock_credentials,
                spreadsheet_id='test-sheet-123'
            )
            # Mock the service
            client.service = Mock()
            yield client

    def test_get_all_articles(self, sheets_client):
        """Test retrieving all articles"""
        # Mock response
        mock_response = {
            'values': [
                ['article_id', 'title', 'stage'],
                ['001', 'Test Article', 'sp_complete'],
                ['002', 'Another Article', 'not_started']
            ]
        }
        sheets_client.service.spreadsheets().values().get().execute.return_value = mock_response

        articles = sheets_client.get_all_articles()

        assert len(articles) == 2
        assert articles[0]['article_id'] == '001'
        assert articles[0]['stage'] == 'sp_complete'

    def test_update_source_status_retry(self, sheets_client):
        """Test retry on transient failures"""
        # First call fails, second succeeds
        sheets_client.service.spreadsheets().values().update().execute.side_effect = [
            ConnectionError("Network error"),
            {'updatedCells': 1}
        ]

        # Should retry and succeed
        sheets_client.update_source_status('SP-001', 'downloaded', 'https://drive.google.com/...')

        # Verify it was called twice
        assert sheets_client.service.spreadsheets().values().update().execute.call_count == 2

    def test_invalid_credentials_error(self):
        """Test error handling for invalid credentials"""
        with pytest.raises(ValueError, match="credentials"):
            SheetsClient(
                credentials_path='/nonexistent/creds.json',
                spreadsheet_id='test-123'
            )
```

### 7.3 Priority 3: GUI Tests

**File:** `/home/user/slr/tests/gui/test_main_window.py`

```python
"""Tests for main window GUI"""
import pytest
from PyQt6.QtWidgets import QApplication
from PyQt6.QtTest import QTest
from PyQt6.QtCore import Qt
from app.gui.main_window import MainWindow

@pytest.fixture(scope='session')
def qapp():
    """Create QApplication for testing"""
    app = QApplication.instance()
    if app is None:
        app = QApplication([])
    yield app

class TestMainWindow:
    """Test main window functionality"""

    @pytest.fixture
    def main_window(self, qapp, mock_config):
        """Create main window for testing"""
        window = MainWindow()
        yield window
        window.close()

    def test_window_initialization(self, main_window):
        """Test window initializes correctly"""
        assert main_window.windowTitle() == "SLR Citation Processor"
        assert main_window.isVisible()

    def test_tab_switching(self, main_window):
        """Test switching between tabs"""
        # Switch to R1 tab
        main_window.tab_widget.setCurrentIndex(1)
        assert main_window.tab_widget.currentIndex() == 1

        # Switch to R2 tab
        main_window.tab_widget.setCurrentIndex(2)
        assert main_window.tab_widget.currentIndex() == 2

    def test_settings_dialog_opens(self, main_window, qtbot):
        """Test settings dialog can be opened"""
        # Trigger settings action
        main_window.settings_action.trigger()

        # Wait for dialog
        qtbot.waitUntil(lambda: main_window.settings_dialog is not None)

        assert main_window.settings_dialog.isVisible()
```

### 7.4 Priority 4: Integration Tests

**File:** `/home/user/slr/tests/integration/test_cache_integration.py`

```python
"""Integration tests for caching across components"""
import pytest
from app.core.sp_machine import SPMachine
from SLRinator.src.utils.cache_manager import CacheManager

class TestCacheIntegration:
    """Test cache integration with pipeline"""

    def test_sp_uses_cache(self, tmp_path, mock_sheets_client, mock_drive_client):
        """Test SP machine uses cache correctly"""
        cache_dir = tmp_path / "cache"
        sp = SPMachine(mock_sheets_client, mock_drive_client, str(cache_dir))

        # First run - should cache
        result1 = sp.process_article('test-001')

        # Second run - should use cache
        result2 = sp.process_article('test-001')

        # Verify cache was used (faster execution, etc.)
        cache = CacheManager(str(cache_dir))
        stats = cache.get_statistics()
        assert stats['hits'] > 0

    def test_cache_invalidation(self, tmp_path, mock_sheets_client, mock_drive_client):
        """Test cache invalidation when data changes"""
        cache_dir = tmp_path / "cache"
        sp = SPMachine(mock_sheets_client, mock_drive_client, str(cache_dir))

        # Process with initial data
        sp.process_article('test-001')

        # Change source data
        mock_sheets_client.get_sources_for_article.return_value[0]['citation'] = 'Updated Citation'

        # Should detect change and re-process
        sp.process_article('test-001')

        # Verify cache was invalidated
```

### 7.5 Priority 5: E2E Tests

**File:** `/home/user/slr/tests/e2e/test_full_pipeline.py`

```python
"""End-to-end tests for full pipeline"""
import pytest
from app.core.orchestrator import PipelineOrchestrator

class TestFullPipeline:
    """Test complete pipeline workflows"""

    @pytest.mark.slow
    @pytest.mark.e2e
    def test_new_article_complete_flow(self, test_config, test_article):
        """Test processing a new article from start to finish"""
        orchestrator = PipelineOrchestrator(
            sheets_client=test_config['sheets'],
            drive_client=test_config['drive'],
            llm_client=test_config['llm'],
            cache_dir=test_config['cache_dir']
        )

        # Run full pipeline
        results = orchestrator.run_full_pipeline(
            test_article['article_id'],
            test_article['word_doc_path']
        )

        # Verify all stages completed
        assert results['sp']['success_count'] > 0
        assert results['r1']['success_count'] > 0
        assert results['r2']['citations_checked'] > 0

        # Verify outputs exist
        assert results['r2']['html_report'] is not None
        assert results['r2']['annotated_pdfs'] is not None

    @pytest.mark.slow
    @pytest.mark.e2e
    def test_resume_from_failure(self, test_config, test_article):
        """Test resuming pipeline after failure"""
        orchestrator = PipelineOrchestrator(
            sheets_client=test_config['sheets'],
            drive_client=test_config['drive'],
            llm_client=test_config['llm'],
            cache_dir=test_config['cache_dir']
        )

        # Simulate failure during R1
        with patch.object(orchestrator.r1_machine, 'process_article') as mock_r1:
            mock_r1.side_effect = RuntimeError("Simulated failure")

            with pytest.raises(RuntimeError):
                orchestrator.run_full_pipeline(test_article['article_id'], test_article['word_doc_path'])

        # Resume should pick up from where it left off
        results = orchestrator.run_full_pipeline(test_article['article_id'], test_article['word_doc_path'])

        # Should complete successfully
        assert results['r1']['success_count'] > 0
```

---

## 8. Implementation Roadmap

### Phase 1: Foundation (Weeks 1-2) ğŸ”´ Critical

**Goal:** Establish testing infrastructure and critical utility tests

**Tasks:**
1. âœ… Create pytest configuration
   - [ ] Create `/home/user/slr/pytest.ini`
   - [ ] Create `/home/user/slr/tests/conftest.py`
   - [ ] Set up coverage configuration

2. âœ… Implement core fixtures
   - [ ] Mock Google Sheets client
   - [ ] Mock Google Drive client
   - [ ] Mock LLM clients
   - [ ] Sample test data (citations, PDFs, articles)

3. âœ… Write utility tests (Priority 1)
   - [ ] test_error_handler.py (15 tests)
   - [ ] test_cache_manager.py (12 tests)
   - [ ] test_retry_handler.py (10 tests)
   - [ ] test_connection_pool.py (8 tests)
   - [ ] test_performance_monitor.py (6 tests)

4. âœ… Set up CI/CD
   - [ ] GitHub Actions workflow
   - [ ] Pre-commit hooks
   - [ ] Coverage reporting

**Deliverables:**
- 51 new utility tests
- Testing infrastructure complete
- CI/CD pipeline running

### Phase 2: Core Components (Weeks 3-4) ğŸŸ¡ High

**Goal:** Test core business logic and data layer

**Tasks:**
1. âœ… Data layer tests
   - [ ] test_sheets_client.py (15 tests)
   - [ ] test_drive_client.py (10 tests)
   - [ ] test_llm_client.py (16 tests - OpenAI + Anthropic)
   - [ ] test_cache_manager_integration.py (8 tests)

2. âœ… Core module tests
   - [ ] test_citation_parser.py (20 tests)
   - [ ] test_pdf_retriever.py (15 tests)
   - [ ] test_source_identifier.py (10 tests)
   - [ ] test_sourcepull_system.py (12 tests)

3. âœ… Processor tests
   - [ ] test_footnote_extractor.py (10 tests)
   - [ ] test_redboxer.py (12 tests)
   - [ ] test_quote_checker.py (8 tests)

**Deliverables:**
- 136 new tests
- 70% core code coverage

### Phase 3: Integration & GUI (Weeks 5-6) ğŸŸ¡ High

**Goal:** Test component integration and GUI

**Tasks:**
1. âœ… Integration tests
   - [ ] test_sp_r1_integration.py (8 tests)
   - [ ] test_r1_r2_integration.py (8 tests)
   - [ ] test_cache_pipeline_integration.py (6 tests)
   - [ ] test_api_integration.py (12 tests)

2. âœ… GUI tests
   - [ ] test_main_window.py (8 tests)
   - [ ] test_sp_manager.py (6 tests)
   - [ ] test_r1_manager.py (6 tests)
   - [ ] test_r2_manager.py (8 tests)
   - [ ] test_settings_dialog.py (5 tests)
   - [ ] test_workers.py (5 tests)

**Deliverables:**
- 72 new tests
- GUI coverage at 60%
- Integration test suite complete

### Phase 4: E2E & Performance (Weeks 7-8) ğŸŸ¢ Medium

**Goal:** End-to-end workflows and performance testing

**Tasks:**
1. âœ… E2E tests
   - [ ] test_full_pipeline.py (6 tests)
   - [ ] test_error_recovery.py (5 tests)
   - [ ] test_resume_capability.py (4 tests)

2. âœ… Performance tests
   - [ ] test_load_performance.py (5 tests - 100+ sources)
   - [ ] test_concurrent_processing.py (3 tests)
   - [ ] test_memory_usage.py (3 tests)

3. âœ… Edge case tests
   - [ ] test_edge_cases_citations.py (15 tests)
   - [ ] test_edge_cases_pdfs.py (10 tests)
   - [ ] test_security_validation.py (8 tests)

**Deliverables:**
- 59 new tests
- Performance benchmarks established
- Security testing complete

### Phase 5: Regression & Documentation (Week 9) ğŸŸ¢ Medium

**Goal:** Regression suite and testing documentation

**Tasks:**
1. âœ… Regression tests
   - [ ] Baseline test data snapshots
   - [ ] Performance regression tests
   - [ ] API compatibility tests

2. âœ… Documentation
   - [ ] Testing guide for developers
   - [ ] Test data management guide
   - [ ] CI/CD documentation
   - [ ] Coverage reports

3. âœ… Quality gates
   - [ ] Code review checklist
   - [ ] Release testing procedure
   - [ ] Bug triage process

**Deliverables:**
- Complete regression suite
- Testing documentation
- Quality processes defined

---

## 9. Test Execution Strategy

### 9.1 Test Organization by Speed

```python
# pytest.ini

[pytest]
markers =
    slow: marks tests as slow (> 1s)
    fast: marks tests as fast (< 100ms)
    integration: integration tests
    e2e: end-to-end tests
    gui: GUI tests requiring display
    network: tests requiring network access

# Run only fast tests during development
# pytest -m fast

# Run all except slow tests in CI
# pytest -m "not slow"

# Run full suite for releases
# pytest
```

### 9.2 Parallel Test Execution

```bash
# Run tests in parallel (8 workers)
pytest -n 8 tests/unit/

# Run GUI tests sequentially (can't parallelize)
pytest tests/gui/

# Run E2E tests with timeout
pytest tests/e2e/ --timeout=600
```

### 9.3 Test Prioritization

**Pre-commit:** Fast unit tests only (~30s)
```bash
pytest -m fast --maxfail=1
```

**Pull Request:** Unit + Integration (~5 min)
```bash
pytest tests/unit/ tests/integration/ -n 4
```

**Nightly Build:** Full suite (~20 min)
```bash
pytest --cov=app --cov=SLRinator --cov=r2_pipeline
```

**Release:** Full suite + E2E + Performance (~45 min)
```bash
pytest --cov=app --cov=SLRinator --cov-report=html
```

---

## 10. Success Metrics

### 10.1 Quantitative Metrics

| Metric | Current | Target | Timeline |
|--------|---------|--------|----------|
| **Test Count** | 41 | 318+ | 9 weeks |
| **Line Coverage** | ~40% | 80%+ | 9 weeks |
| **Branch Coverage** | Unknown | 75%+ | 9 weeks |
| **Test Execution Time** | ~30s | <5 min | Ongoing |
| **CI Pass Rate** | N/A | >95% | After CI setup |
| **Defect Escape Rate** | Unknown | <5% | Ongoing tracking |

### 10.2 Qualitative Metrics

- âœ… All critical paths have tests
- âœ… No production incidents from untested code
- âœ… Developers confident in refactoring
- âœ… Fast feedback on code changes (<5 min)
- âœ… Regression suite prevents breaking changes

---

## 11. Risk Assessment

### 11.1 Testing Risks

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| **Low test coverage allows bugs** | High | High | Prioritize critical path tests first |
| **Flaky tests slow CI** | Medium | Medium | Implement retry logic, mock external services |
| **GUI tests break frequently** | Medium | Low | Use page object pattern, stable selectors |
| **E2E tests too slow** | High | Medium | Run nightly, not on every commit |
| **Test data becomes stale** | Medium | Medium | Regular test data refresh process |
| **Mocks diverge from reality** | Medium | High | Contract testing, periodic integration tests |

### 11.2 Implementation Risks

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| **Takes too long to implement** | Medium | High | Phased approach, prioritize critical tests |
| **Developer resistance** | Low | Medium | Show value early, automate everything |
| **Maintenance burden** | Medium | Medium | DRY principles, good fixtures |
| **False sense of security** | Low | High | Regular code reviews, mutation testing |

---

## 12. Recommendations Summary

### Immediate Actions (Week 1)

1. **Create pytest configuration**
   ```bash
   cd /home/user/slr
   touch pytest.ini
   touch tests/conftest.py
   ```

2. **Set up coverage tracking**
   ```bash
   pip install pytest-cov coverage
   pytest --cov=app --cov=SLRinator --cov-report=html
   ```

3. **Implement core fixtures** (see Section 5.1)

4. **Write first 20 utility tests** (error_handler, cache_manager)

### Short-term Goals (Months 1-2)

1. Reach 65% code coverage
2. Implement CI/CD pipeline
3. Complete all utility and data layer tests
4. Set up pre-commit hooks

### Long-term Goals (Months 3+)

1. Reach 80%+ code coverage
2. Full E2E test suite
3. Performance benchmarking
4. Mutation testing for test quality

---

## Appendices

### Appendix A: Test File Locations

All new test files should be created in this structure:

```
/home/user/slr/tests/
â”œâ”€â”€ conftest.py
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_utils/
â”‚   â”‚   â”œâ”€â”€ test_error_handler.py
â”‚   â”‚   â”œâ”€â”€ test_cache_manager.py
â”‚   â”‚   â”œâ”€â”€ test_retry_handler.py
â”‚   â”‚   â”œâ”€â”€ test_connection_pool.py
â”‚   â”‚   â””â”€â”€ test_performance_monitor.py
â”‚   â”œâ”€â”€ test_data/
â”‚   â”‚   â”œâ”€â”€ test_sheets_client.py
â”‚   â”‚   â”œâ”€â”€ test_drive_client.py
â”‚   â”‚   â””â”€â”€ test_llm_client.py
â”‚   â”œâ”€â”€ test_core/
â”‚   â”‚   â”œâ”€â”€ test_sp_machine.py
â”‚   â”‚   â”œâ”€â”€ test_r1_machine.py
â”‚   â”‚   â””â”€â”€ test_r2_pipeline.py
â”‚   â””â”€â”€ test_processors/
â”‚       â”œâ”€â”€ test_footnote_extractor.py
â”‚       â””â”€â”€ test_redboxer.py
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_pipeline_integration.py
â”‚   â”œâ”€â”€ test_cache_integration.py
â”‚   â””â”€â”€ test_api_integration.py
â”œâ”€â”€ gui/
â”‚   â”œâ”€â”€ test_main_window.py
â”‚   â”œâ”€â”€ test_sp_manager.py
â”‚   â”œâ”€â”€ test_r1_manager.py
â”‚   â””â”€â”€ test_r2_manager.py
â””â”€â”€ e2e/
    â”œâ”€â”€ test_full_pipeline.py
    â””â”€â”€ test_error_recovery.py
```

### Appendix B: Coverage Configuration

Create `/home/user/slr/.coveragerc`:

```ini
[run]
source = app,SLRinator,r2_pipeline
omit =
    */tests/*
    */test_*.py
    */__pycache__/*
    */venv/*
    */site-packages/*
    */conftest.py

[report]
precision = 2
show_missing = True
skip_covered = False

exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod

[html]
directory = htmlcov
```

### Appendix C: pytest Configuration

Create `/home/user/slr/pytest.ini`:

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

addopts =
    -v
    --strict-markers
    --tb=short
    --disable-warnings
    --cov-report=term-missing
    --cov-report=html
    --cov-report=xml

markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    fast: marks tests as fast
    integration: integration tests
    e2e: end-to-end tests
    gui: GUI tests
    network: tests requiring network
    unit: unit tests

filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning

timeout = 300
```

---

**Report prepared by:** QA & Testing Expert
**Date:** 2025-11-16
**Version:** 1.0
**Status:** Ready for Implementation
